# Instalar as dependências no Colab
!apt-get install -y python-opengl ffmpeg > /dev/null
!pip install gymnasium moviepy pygame > /dev/null

import os
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt
from moviepy.editor import ImageSequenceClip
from IPython.display import Image, display
from PIL import Image as PILImage

# Rede Neural para aproximar os Q-values
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Função para converter estado discreto em one-hot encoding
def one_hot_encode(state, n_states):
    one_hot = np.zeros(n_states)
    one_hot[state] = 1
    return one_hot

# Função para escolher a ação usando epsilon-greedy
def epsilon_greedy(state, q_network, epsilon, action_size):
    if random.random() < epsilon:
        return random.randint(0, action_size - 1)
    else:
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = q_network(state_tensor)
            return torch.argmax(q_values).item()

# Função de treinamento
def train_dqn(env, n_episodes=1000, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, alpha=0.001, batch_size=64):
    n_states = env.observation_space.n
    action_size = env.action_space.n

    # Inicializar a rede neural e o otimizador
    q_network = DQN(n_states, action_size)
    target_network = DQN(n_states, action_size)
    target_network.load_state_dict(q_network.state_dict())
    optimizer = optim.Adam(q_network.parameters(), lr=alpha)
    loss_fn = nn.MSELoss()

    replay_buffer = deque(maxlen=10000)
    rewards_per_episode = []

    for episode in range(n_episodes):
        state, _ = env.reset()
        state = one_hot_encode(state, n_states)  # Converter estado para one-hot
        total_reward = 0
        done = False

        while not done:
            action = epsilon_greedy(state, q_network, epsilon, action_size)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_reward += reward

            next_state = one_hot_encode(next_state, n_states)  # Converter próximo estado para one-hot

            # Armazenar transição no replay buffer
            replay_buffer.append((state, action, reward, next_state, done))
            state = next_state

            # Treinar a rede neural se o buffer tiver exemplos suficientes
            if len(replay_buffer) >= batch_size:
                batch = random.sample(replay_buffer, batch_size)
                train_batch(q_network, target_network, optimizer, loss_fn, batch, gamma, n_states)

        # Decaimento do epsilon
        epsilon = max(epsilon * epsilon_decay, epsilon_min)
        rewards_per_episode.append(total_reward)

        # Atualizar a rede-alvo periodicamente
        if episode % 10 == 0:
            target_network.load_state_dict(q_network.state_dict())

    return q_network, rewards_per_episode

# Função de treinamento do batch
def train_batch(q_network, target_network, optimizer, loss_fn, batch, gamma, n_states):
    states, actions, rewards, next_states, dones = zip(*batch)

    states = torch.FloatTensor(states)
    next_states = torch.FloatTensor(next_states)
    actions = torch.LongTensor(actions)
    rewards = torch.FloatTensor(rewards)
    dones = torch.FloatTensor(dones)

    q_values = q_network(states)
    next_q_values = target_network(next_states)

    q_values_selected = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
    q_targets = rewards + (1 - dones) * gamma * next_q_values.max(1)[0]

    loss = loss_fn(q_values_selected, q_targets)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Função para capturar os quadros renderizados e salvar em arquivos
def capture_frames(env, q_network, episodes_to_run=5):
    frames = []
    n_states = env.observation_space.n

    for episode in range(episodes_to_run):
        state, _ = env.reset()  # Resetando o ambiente
        state = one_hot_encode(state, n_states)  # Converter estado para one-hot
        done = False
        while not done:
            action = epsilon_greedy(state, q_network, epsilon=0, action_size=env.action_space.n)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            next_state = one_hot_encode(next_state, n_states)  # Converter próximo estado para one-hot

            # Capturar o quadro renderizado
            frame = env.render()  # Agora com 'render()' correto, sem 'mode' se estiver com o render_mode certo
            frames.append(frame)
            state = next_state

    return frames


# Função para criar GIF a partir dos quadros
def create_gif(frames, filename, fps=2):
    clip = ImageSequenceClip(frames, fps=fps)
    clip.write_gif(filename, fps=fps)

# Código principal
if __name__ == "__main__":
    if not os.path.exists("images"):
        os.makedirs("images")
    
    env = gym.make("Taxi-v3", render_mode="rgb_array")

    # Treinamento do DQN
    q_network, rewards_per_episode = train_dqn(env, n_episodes=500)

    # Visualizar métricas
    plt.plot(rewards_per_episode)
    plt.title("Rewards Per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.show()

    # Visualizar o agente
    frames = capture_frames(env, q_network, episodes_to_run=5)

    # Criar GIF
    gif_path = "images/dqn_training_visualization.gif"
    create_gif(frames, gif_path, fps=2)

    # Exibir o GIF no Colab
    display(Image(gif_path))
